{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1-2**\n",
    "\n",
    "The average value of v_min is closest to 0.01 and both v_1 and v_rand satisfy the single-bin Hoeffding Inequality (i.e. v is close to u)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_1: 0.5016000000000006\n",
      "v_rand: 0.4994000000000002\n",
      "v_min: 0.03740000000000026\n"
     ]
    }
   ],
   "source": [
    "import random as rd\n",
    "import numpy as np\n",
    "\n",
    "class CoinFlip:\n",
    "    def __init__(self, N_coins=1000, N_flips=10):\n",
    "        self.N_coins = N_coins\n",
    "        self.N_flips = N_flips\n",
    "        self.flips = np.random.randint(2, size=(N_coins, N_flips))\n",
    "        self.heads = np.sum(self.flips, axis=1)\n",
    "\n",
    "    def v_1(self):\n",
    "        '''\n",
    "        returns the fraction of heads for the first coin\n",
    "        '''\n",
    "\n",
    "        return float(self.heads[0]) / self.N_flips\n",
    "\n",
    "    def v_rand(self):\n",
    "        '''\n",
    "        returns the fraction of heads for a random coin\n",
    "        '''\n",
    "\n",
    "        coin_index = rd.randint(0, self.N_coins - 1)\n",
    "        return float(self.heads[coin_index]) / self.N_flips\n",
    "\n",
    "    def v_min(self):\n",
    "        '''\n",
    "        returns the lowest fraction of heads across all coins\n",
    "        '''\n",
    "\n",
    "        return float(np.amin(self.heads, axis=0)) / self.N_flips\n",
    "\n",
    "    \n",
    "N_simulations = 1000\n",
    "v_1, v_rand, v_min = 0.0, 0.0, 0.0\n",
    "\n",
    "for _ in range(N_simulations):\n",
    "    flip = CoinFlip(N_coins=1000, N_flips=10)\n",
    "    v_1 += flip.v_1()\n",
    "    v_rand += flip.v_rand()\n",
    "    v_min += flip.v_min()\n",
    "\n",
    "print('v_1: {}'.format(v_1 / N_simulations))\n",
    "print('v_rand: {}'.format(v_rand / N_simulations))\n",
    "print('v_min: {}'.format(v_min / N_simulations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3**\n",
    "\n",
    "You have the case where h incorrectly models f and f correctly determines y, as well as the case where h correctly models f, but f incorrectly determines y. That means the probability error that h makes in approximating y is:\n",
    "\n",
    "$$(1 - \\lambda) \\cdot ( 1 - \\mu) + \\lambda \\cdot \\mu$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**\n",
    "\n",
    "The performance of h is independent of $\\lambda$ when:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mu} (1 - \\lambda) \\cdot ( 1 - \\mu) + \\lambda \\cdot \\mu = 0 \\\\\n",
    "(\\lambda - 1) + \\lambda = 0\\\\\n",
    "\\lambda = \\frac{1}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5-6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Ein: 0.03861000000000003\n",
      "Avg. Eout: 0.04788600000000003\n"
     ]
    }
   ],
   "source": [
    "from lfd import error_rate, classify, Data, OLS, Line\n",
    "\n",
    "N_simulations = 1000\n",
    "N_train = 100\n",
    "N_test = 1000\n",
    "\n",
    "E_in, E_out = 0.0, 0.0\n",
    "\n",
    "for _ in range(N_simulations):\n",
    "    line = Line()\n",
    "\n",
    "    training_set = Data([N_train, 3], intercept=True)\n",
    "    training_set.Y = classify(training_set.X, line.weights)\n",
    "\n",
    "    test_set = Data([N_test, 3], intercept=True)\n",
    "    test_set.Y = classify(test_set.X, line.weights)\n",
    "\n",
    "    linreg = OLS()\n",
    "    linreg.run(training_set.X, training_set.Y)\n",
    "    weights = linreg.weights\n",
    "\n",
    "    prediction_in = classify(training_set.X, weights)\n",
    "    E_in += error_rate(prediction_in, training_set.Y)\n",
    "\n",
    "    prediction_out = classify(test_set.X, weights)\n",
    "    E_out += error_rate(prediction_out, test_set.Y)\n",
    "\n",
    "print('Avg. Ein: {}'.format(E_in / N_simulations))\n",
    "print('Avg. Eout: {}'.format(E_out / N_simulations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7**\n",
    "\n",
    "Initializing the Perceptron weights with the least squares coefficients reduces the number of iterations required for convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. PLA iterations: 4.145\n"
     ]
    }
   ],
   "source": [
    "from lfd import PLA\n",
    "\n",
    "N_simulations = 1000\n",
    "N_train = 10\n",
    "iterations = 0.0\n",
    "\n",
    "for _ in range(N_simulations):\n",
    "    line = Line()\n",
    "    training_set = Data([N_train, 3], intercept=True)\n",
    "    training_set.Y = classify(training_set.X, line.weights)\n",
    "\n",
    "    linreg = OLS()\n",
    "    linreg.run(training_set.X, training_set.Y)\n",
    "    weights = linreg.weights\n",
    "    perceptron = PLA(weights=weights)\n",
    "    iterations += perceptron.run(training_set.X, training_set.Y)\n",
    "\n",
    "print('Avg. PLA iterations: {}'.format(iterations / N_simulations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Ein: 0.4424599999999999\n"
     ]
    }
   ],
   "source": [
    "N_simulations = 1000\n",
    "N_train = 100\n",
    "N_test = 1000\n",
    "\n",
    "E_in, E_out = 0.0, 0.0\n",
    "\n",
    "for _ in range(N_simulations):\n",
    "    line = Line()\n",
    "    training_set = Data([N_train, 3], intercept=True)\n",
    "\n",
    "    training_set.Y = np.sign(\n",
    "            np.add(\n",
    "                np.multiply(training_set.X[:, 1], training_set.X[:, 1]),\n",
    "                np.multiply(training_set.X[:, 2], training_set.X[:, 2]),\n",
    "            ) - 0.6\n",
    "        )\n",
    "\n",
    "    training_set.add_noise()\n",
    "\n",
    "    linreg = OLS()\n",
    "    linreg.run(training_set.X, training_set.Y)\n",
    "\n",
    "    weights = linreg.weights\n",
    "    prediction = classify(training_set.X, weights)\n",
    "    E_in += error_rate(prediction, training_set.Y)\n",
    "\n",
    "print('Avg. Ein: {}'.format(E_in / N_simulations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9**\n",
    "\n",
    "The weights found are close to [-1, -0.05, 0.08, 0.13, 1.5, 1.5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. hypothesis weights: [ -9.91887999e-01   1.96334774e-03   3.60558872e-04  -6.36692467e-04\n",
      "   1.55761458e+00   1.55646690e+00]\n"
     ]
    }
   ],
   "source": [
    "N_simulations = 1000\n",
    "N_train = 1000\n",
    "\n",
    "E_in, E_out = 0.0, 0.0\n",
    "weights = []\n",
    "\n",
    "for _ in range(N_simulations):\n",
    "    training_set = Data([N_train, 3], intercept=True)\n",
    "    training_set.add_columns([\n",
    "            np.multiply(training_set.X[:, 1], training_set.X[:, 2]),\n",
    "            np.multiply(training_set.X[:, 1], training_set.X[:, 1]),\n",
    "            np.multiply(training_set.X[:, 2], training_set.X[:, 2])\n",
    "    ])\n",
    "\n",
    "    training_set.Y = classify(training_set.X, weights=[-0.6, 0, 0, 0, 1, 1])\n",
    "    training_set.add_noise()\n",
    "\n",
    "    linreg = OLS()\n",
    "    linreg.run(training_set.X, training_set.Y)\n",
    "    weights.append(linreg.weights)\n",
    "\n",
    "print('Avg. hypothesis weights: {}'.format(np.mean(weights, axis=0)))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Eout: 0.053940999999999795\n"
     ]
    }
   ],
   "source": [
    "N_simulations = 1000\n",
    "N_test = 1000\n",
    "\n",
    "E_in, E_out = 0.0, 0.0\n",
    "\n",
    "for _ in range(N_simulations):\n",
    "    test_set = Data([N_test, 3], intercept=True)\n",
    "    test_set.add_columns([\n",
    "            np.multiply(test_set.X[:, 1], test_set.X[:, 2]),\n",
    "            np.multiply(test_set.X[:, 1], test_set.X[:, 1]),\n",
    "            np.multiply(test_set.X[:, 2], test_set.X[:, 2])\n",
    "    ])\n",
    "\n",
    "    test_set.Y = classify(test_set.X, weights=[-0.6, 0, 0, 0, 1, 1])\n",
    "    prediction = classify(test_set.X, \n",
    "                          weights=[-1, -0.05, 0.08, 0.13, 1.5, 1.5])\n",
    "    E_out += error_rate(prediction, test_set.Y)\n",
    "\n",
    "print('Avg. Eout: {}'.format(E_out / N_simulations))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

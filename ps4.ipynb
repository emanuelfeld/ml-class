{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**\n",
    "\n",
    "The solution solves: \n",
    "\n",
    "$$\\mathbb{P}[|E_{out}-E_{in}|>\\epsilon] \\leq 4(2N)^{d_{vc}}\\mathcal{e}^{-\\frac{1}{8}\\epsilon^2N} \\leq 0.05$$\n",
    "\n",
    "for $N$, where $\\epsilon=0.05$, $d_{vc}=10$, and $N>d_{vc}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 452956.8647231])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from scipy.optimize import fsolve\n",
    "\n",
    "def mH(N, dvc):\n",
    "    return float(N)**dvc\n",
    "\n",
    "def generalize(epsilon, dvc, err):\n",
    "    def f(N):\n",
    "        return (\n",
    "            4 * mH(2*N, dvc) \n",
    "            * math.exp(-1 / 8 * epsilon**2 * float(N)) \n",
    "            - err\n",
    "        )\n",
    "    return f\n",
    "\n",
    "f = generalize(0.05, 10, 0.05)\n",
    "\n",
    "fsolve(f, 400000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2**\n",
    "\n",
    "For a very large N, Devroye provides the tightest bound on the generalization error, $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original VC bound (N = 10,000): [ 0.63217492]\n",
      "Rademacher penalty bound (N = 10,000): [ 0.3331727]\n",
      "Parrondo and Van den Broek bound (N = 10,000): [ 0.22369829]\n",
      "Devroye bound (N = 10,000): [ 0.21522805]\n"
     ]
    }
   ],
   "source": [
    "def bounds(N, dvc, delta):\n",
    "    def og_vc(epsilon):\n",
    "        return (\n",
    "            math.sqrt(8 / N * math.log(4 * mH(2*N, dvc) / delta)) \n",
    "            - epsilon\n",
    "        )\n",
    "    def rademacher(epsilon):\n",
    "        return (\n",
    "            math.sqrt(2 * math.log(2 * N * mH(N, dvc)) / N) \n",
    "            + math.sqrt(2 / N * math.log(1 / delta) + 1 / N) \n",
    "            - epsilon\n",
    "        )\n",
    "    def parrondo(epsilon):\n",
    "        return (\n",
    "            math.sqrt(1 / N * \n",
    "                      (2 * epsilon \n",
    "                       + math.log(6 * mH(2*N, dvc) / delta))\n",
    "                     ) \n",
    "            - epsilon\n",
    "        )\n",
    "    def devroye(epsilon):\n",
    "        return (\n",
    "            math.sqrt(\n",
    "                1 / (2*N) * (\n",
    "                    4 * epsilon * (1 + epsilon) \n",
    "                    + math.log(4/delta) + 2*dvc*math.log(N))\n",
    "            ) \n",
    "            - epsilon\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        \"og_vc\": og_vc,\n",
    "        \"rademacher\": rademacher,\n",
    "        \"parrondo\": parrondo,\n",
    "        \"devroye\": devroye\n",
    "    }\n",
    "        \n",
    "b = bounds(10000.0, 50, 0.05)\n",
    "\n",
    "print('Original VC bound (N = 10,000): {}'.format(\n",
    "        fsolve(b['og_vc'], 0)))\n",
    "print('Rademacher penalty bound (N = 10,000): {}'.format(\n",
    "        fsolve(b['rademacher'], 0)))\n",
    "print('Parrondo and Van den Broek bound (N = 10,000): {}'.format(\n",
    "        fsolve(b['parrondo'], 0)))\n",
    "print('Devroye bound (N = 10,000): {}'.format(\n",
    "        fsolve(b['devroye'], 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3**\n",
    "\n",
    "For small N, the least bound on the generalization error is given by Parrondo and Van den Broek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original VC bound (N = 5): [ 13.82816148]\n",
      "Rademacher penalty bound (N = 5): [ 6.93660526]\n",
      "Parrondo and Van den Broek bound (N = 5): [ 5.10136198]\n",
      "Devroye bound (N = 5): [ 5.59312554]\n"
     ]
    }
   ],
   "source": [
    "b = bounds(5.0, 50, 0.05)\n",
    "\n",
    "print('Original VC bound (N = 5): {}'.format(\n",
    "        fsolve(b['og_vc'], 0)))\n",
    "print('Rademacher penalty bound (N = 5): {}'.format(\n",
    "        fsolve(b['rademacher'], 0)))\n",
    "print('Parrondo and Van den Broek bound (N = 5): {}'.format(\n",
    "        fsolve(b['parrondo'], 0)))\n",
    "print('Devroye bound (N = 5): {}'.format(\n",
    "        fsolve(b['devroye'], 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected value of the hypothesis is: 1.43x\n"
     ]
    }
   ],
   "source": [
    "from lfd import Data, OLS\n",
    "import numpy as np\n",
    "import random as rd\n",
    "\n",
    "N_train = 2\n",
    "N_simulations = 10000\n",
    "\n",
    "weights = []\n",
    "X_list, Y_list = [], []\n",
    "gD_list = []\n",
    "\n",
    "for _ in range(N_simulations):\n",
    "    training_set = Data([N_train, 1], intercept=False)\n",
    "    training_set.Y = np.array([np.sin(np.pi * row[0]) for row in training_set.X])\n",
    "\n",
    "    linreg = OLS()\n",
    "    # regress X on Y with intercept set to 0.\n",
    "    linreg.run(training_set.X, training_set.Y)\n",
    "\n",
    "    weights.append(linreg.weights[0])\n",
    "    \n",
    "    # for later:\n",
    "    gD_list.extend((linreg.weights[0] * training_set.X))\n",
    "    X_list.extend(training_set.X)\n",
    "    Y_list.extend(training_set.Y)\n",
    "\n",
    "mean_weight = np.mean(weights)\n",
    "\n",
    "print('The expected value of the hypothesis is: {}x'.format(\n",
    "        round(mean_weight, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5-6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bias is: 0.2714601164590224\n",
      "The variance is: 0.20438896192084355\n"
     ]
    }
   ],
   "source": [
    "variance_list = []\n",
    "bias_list = []\n",
    "\n",
    "for i in range(len(X_list)):\n",
    "    point_var = (gD_list[i] - mean_weight * X_list[i])**2\n",
    "    point_bias = (Y_list[i] - mean_weight * X_list[i])**2\n",
    "    variance_list.append(point_var)\n",
    "    bias_list.append(point_bias)\n",
    "\n",
    "print('The bias is: {}'.format(np.mean(bias_list)))\n",
    "print('The variance is: {}'.format(np.mean(variance_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.507375568176\n",
      "0.270689796785\n",
      "0.208903640837\n",
      "0.512868032904\n",
      "4.9585857921\n"
     ]
    }
   ],
   "source": [
    "a_weights = []\n",
    "b_weights = []\n",
    "c_weights = []\n",
    "d_weights = []\n",
    "e_weights = []\n",
    "\n",
    "\n",
    "def mse(hypothesis, actual):\n",
    "    return np.mean(np.square(np.subtract(hypothesis, actual)))\n",
    "\n",
    "\n",
    "for _ in range(N_simulations):\n",
    "    training_set = Data([N_train, 2], intercept=True)\n",
    "    training_set.Y = np.array([np.sin(np.pi * row[1]) for row in training_set.X])\n",
    "\n",
    "    # add an x^2 column\n",
    "    training_set.add_columns([\n",
    "            np.multiply(training_set.X[:, 1], training_set.X[:, 1])\n",
    "        ])\n",
    "\n",
    "    linreg = OLS()\n",
    "    \n",
    "    # h(x) = b\n",
    "    linreg.run(training_set.X[:, [0]], training_set.Y)\n",
    "    a_weights.append(linreg.weights)\n",
    "    \n",
    "    # h(x) = ax\n",
    "    linreg.run(training_set.X[:, [1]], training_set.Y)\n",
    "    b_weights.append(linreg.weights)\n",
    "    \n",
    "    # h(x) = ax + b\n",
    "    linreg.run(training_set.X[:, [0, 1]], training_set.Y)\n",
    "    c_weights.append(linreg.weights)\n",
    "    \n",
    "    # h(x) = ax^2\n",
    "    linreg.run(training_set.X[:, [2]], training_set.Y)\n",
    "    d_weights.append(linreg.weights)\n",
    "\n",
    "    # h(x) = ax^2 + b\n",
    "    linreg.run(training_set.X[:, [0, 2]], training_set.Y)\n",
    "    e_weights.append(linreg.weights)\n",
    "\n",
    "a_weights = np.mean(a_weights, axis=0)\n",
    "b_weights = np.mean(b_weights, axis=0)\n",
    "c_weights = np.mean(c_weights, axis=0)\n",
    "d_weights = np.mean(d_weights, axis=0)\n",
    "e_weights = np.mean(e_weights, axis=0)\n",
    "\n",
    "N_test = 10000\n",
    "\n",
    "test_set = Data([N_test, 2], intercept=True)\n",
    "test_set.Y = np.array([np.sin(np.pi * row[1]) for row in test_set.X])\n",
    "\n",
    "test_set.add_columns([\n",
    "        np.multiply(test_set.X[:, 1], test_set.X[:, 1])\n",
    "    ])\n",
    "\n",
    "print(mse(np.dot(test_set.X[:, [0]], a_weights), test_set.Y))\n",
    "print(mse(np.dot(test_set.X[:, [1]], b_weights), test_set.Y))\n",
    "print(mse(np.dot(test_set.X[:, [0, 1]], c_weights), test_set.Y))\n",
    "print(mse(np.dot(test_set.X[:, [2]], d_weights), test_set.Y))\n",
    "print(mse(np.dot(test_set.X[:, [0, 2]], e_weights), test_set.Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8**\n",
    "\n",
    "$$\\mathcal{m_H}(N+1)=2\\mathcal{m_H}(N)-\\binom{N}{q} \\\\\n",
    "=2(2\\mathcal{m_H}(N-1)-\\binom{N-1}{q})-\\binom{N}{q} \\\\\n",
    "=2^2\\mathcal{m_H}(N-1)-(2\\binom{N-1}{q}+\\binom{N}{q}) \\\\\n",
    "=2^{N}\\mathcal{m_H}(N-(N-1))-(2^{N-1}\\binom{N-(N-1)}{q}+...+2^1\\binom{N-1}{q}+2^0\\binom{N}{q}) \\\\\n",
    "=2^{N}\\mathcal{m_H}(1) - \\sum_{n=0}^{N-1} 2^n \\binom{N-n}{q} \\\\\n",
    "=2^{N} \\cdot 2 -\\sum_{n=0}^{N-1} 2^n \\binom{N-n}{q} \\\\\n",
    "=2^{N+1} -\\sum_{n=0}^{N-1} 2^n \\binom{N-n}{q} \\\\\n",
    "$$\n",
    "\n",
    "Or in terms of $N$:\n",
    "\n",
    "$$\\mathcal{m_H}(N)= 2^{N} -\\sum_{n=0}^{N-2} 2^n \\binom{N-1-n}{q}$$\n",
    "\n",
    "The VC dimension of a hypothesis set $\\mathcal{H}$ is the largest (positive integer) value of $N$ for which $\\mathcal{m_H}(N)=2^N$. In other words, we are looking for the largest $N$ such that:\n",
    "\n",
    "$$\\sum_{n=0}^{N-2} 2^n \\binom{N-1-n}{q}=0$$\n",
    "\n",
    "If $N>q$, this series will have at least one positive term. If $N=q$, all the terms equal zero. So the VC dimension, $d_{vc}(\\mathcal{H})=q$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
